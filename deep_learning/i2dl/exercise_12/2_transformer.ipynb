{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026e0255",
   "metadata": {},
   "source": [
    "# Transformers - Attention is All You Need\n",
    "Welcome to the captivating world of Transformers in Natural Language Processing (NLP)! In recent years, Transformers have emerged as a groundbreaking paradigm in NLP, revolutionizing how we comprehend and process language. Unlike older architectures such as LSTMs (Long Short-Term Memory) and CNNs (Convolutional Neural Networks), Transformers leverage self-attention mechanisms, enabling them to capture long-range dependencies more effectively and allowing for parallelization of computations. The seminal paper ['Attention Is All You Need'](https://arxiv.org/abs/1706.03762) introduced the Transformer model, showcasing its prowess in various language tasks and laying the foundation for its widespread adoption. This Jupyter notebook is designed to unravel the intricacies of Transformers, highlighting their architectural nuances, contrasting them with traditional models, and demonstrating their applications across diverse NLP domains! (And in case you were wondering - yes, a certain transformer model created this text :D)\n",
    "\n",
    "We will stick very closely to the implementation presented in the paper to build our own translation model! There is no need to read it though, you can find the necessary sections of the paper before each task, together with some explanations. If you did not work on exercise 11, we would really advise you to do that first, since we will pick up on many of those topics!\n",
    "\n",
    "Alright - let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c01bc",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc9819",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f568e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util import notebook_util as util\n",
    "from exercise_code.network import *\n",
    "from exercise_code.tests import *\n",
    "from exercise_code.trainer import MPS_AVAILABLE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "root_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "model_path = os.path.join(os.getcwd(), 'models')\n",
    "submission_path = os.path.join(os.getcwd(), 'submission_files')\n",
    "pretrained_model_path = os.path.join(model_path, 'pretrainedModels')\n",
    "dataset_path = os.path.join(root_path, 'datasets', 'transformerDatasets')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a27adc",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Transformer.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer.drawio.png\"  width=\"2500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The model consists of two bigger building blocks, the encoder and the decoder. The encoder processes the input to our model. In our translation model, this input would be the sentence in the source language. The decoder part iteratively produces an output sequence. As an input it takes the already predicted words and given that sequence and the encoder input, it produces an output sequence as follows:\n",
    "\n",
    "| Encoder Input                       | Decoder Input                                      | Decoder Output                                   |\n",
    "|-------------------------------------|----------------------------------------------------|--------------------------------------------------|\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\"]                                       | [\"Hallo\"]                                        |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\"]                              | [\"Hallo\", \"wie\"]                                 |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\"]                       | [\"Hallo\", \"wie\", \"geht's\"]                       |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\"]             | [\"Hallo\", \"wie\", \"geht's\", \"dir\"]                |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\"]      | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"]           |\n",
    "| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"] | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"] |\n",
    "\n",
    "And the predicted end token breaks the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310bd92",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>WARNING: Task Implementation</h3>\n",
    "    <p>Throughout this notebook you will as always have to complete several tasks to complete the individual modules! Please be aware though, that some Modules may have several tasks that have to be completed, but please <strong>only</strong> concentrate on the <strong>current task</strong> and the <strong>corresponding hints</strong> (if there are any;). In other words, if you are working on Task 1, and there is also Task 4 in the TODOs of that Module, you don't have to work on it at this moment! <br>\n",
    "    Also, if we mention any specific pytorch modules in the task description or hints, you <strong>are allowed to use them!</strong> With that said, let's work on your first task!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d0ecf3b6214a5",
   "metadata": {},
   "source": [
    "# Cross Attention vs Self Attention\n",
    "\n",
    "You might have noticed, that we are always talking about inputs to an attention mechanism and the context we want to compare it to. If we use the terminology from the paper, our inputs form the query of our attention mechanism, and the context form the key - value pairs! \n",
    "\n",
    "## Cross Attention\n",
    "Cross-Attention might be the easier idea to understand. We use two different sources for the input and context! In the transformer model, this type of attention will be used in the decoder, that way we can contextualize our output with the actual input to our model. \n",
    "\n",
    "Let's say you ask the model a question like \"Hello, how are you?\". This will be processed by the encoder and it will give us some output. Next, the decoder will start to produce its output token by token, and at each step, the output should obviously depend on our initial question! We can achieve this by using cross attention, where the inputs to the mechanism come from the decoder itself, and the context comes from the encoder. It is trying to give context from our input to its output - something we definetly want to have!\n",
    "\n",
    "## Self Attention\n",
    "In Self-Attention, the input and the context source are the same! That means, every word in a sequence is attending to all words in the same sequence, or in other words - the sentence is attending to itself. This form is used in both the encoder and decoder, and is used to process their inputs to gain a \"deeper\" understanding of the inputs by giving them context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872475",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Self Cross Attention.drawio.png\" width=2000> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Self Cross Attention.drawio.png\" width=2000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f16f75c0b5bae",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "The encoder side of the transformer processes the input to the model. If your model is trained on translating sentences, the input will be the sentence in the source language. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **feed forward network**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "\n",
    "We use layer normalization instead of batch normalization to get similar advantages of improving training stability while decoupling it from the batch size. (Remember, in batch normalization we normalize over an entire batch, while in layer normalization we normalize across over all inputs of a single sample!)\n",
    "\n",
    "The feed forward network is applied to each token embedding separately with follwing architecture: \n",
    "\n",
    "$FFN(x) = RELU(xW_1 + b_1) \\cdot W_2 + b_2$,  where\n",
    "\n",
    "$shape(W_1) = (d_{model},\\, d_{ff})$ </br>\n",
    "$shape(W_2) = (d_{ff},\\, d_{model})$\n",
    "\n",
    "After this, another residual connection followed by a layer normalization is added.\n",
    "\n",
    "One intuitive way to think about this $MultiHead \\rightarrow FFN$ structure of the encoder block is that the attention mechanism is used to \"look around\" in the input sequence and the feed forward network is used to process the information and transform it into a more useful representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1971a5",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Encoder Block.drawio.png\" width=\"1300\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder Block.drawio.png\" width=1300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb92f5506c45c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 3: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>FeedForwardNeuralNetwork</code> class in <code>exercise_code/network/feed_forward_nn.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab94c21859e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ce69f250dbc3e",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-info\">\n",
    "    <h3>Task 4: Implement </h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>EncoderBlock</code> class in <code>exercise_code/network/encoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6263d0344428b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09703b84",
   "metadata": {},
   "source": [
    "## Encoder Stack\n",
    "\n",
    "The only part left on the encoder side is to stack multiple blocks together!\n",
    "\n",
    "<!-- <img src=\"images/Transformer-Encoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Encoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ca607",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 5: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Encoder</code> class in <code>exercise_code/network/encoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74017db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b46041",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The job of the decoder is basically to produce an output given an input and the previous outputs (if available). Those previous outputs are also the input to the decoder! We have actually already seen most of the relevant parts of the decoder in the encoder, there is only one major addition: Causal attention!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b573662",
   "metadata": {},
   "source": [
    "## Causal Attention\n",
    "\n",
    "Previous language models based on RNN have one large draw back: during training, the model has to iteratively go through the entire sequence to predict the next word. Transformer models on the other hand can do this in parallel!\n",
    "\n",
    "So instead of something like this:\n",
    "\n",
    "|                    | Iteration 1                        | Iteration 2                        | Iteration 3                        |  \n",
    "|--------------------|------------------------------------|------------------------------------|------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]| [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]|\n",
    "| **Decoder Input**  | [\"\\<start>\"]                       | [\"\\<start>\", \"Nicht\"]              | [\"\\<start>\", \"Nicht\", \"sehr\"]      | \n",
    "| **Decoder Output** | [\"Nicht\"]                          | [\"Nicht\", \"sehr\"]                  | [\"Nicht\", \"sehr\", \"effektiv\"]      | \n",
    "| **Compare to**     | [\"Hallo\"]                          | [\"Hallo\", \"wie\"]                   | [\"Hallo\", \"wie\", \"geht's\"]         | \n",
    "\n",
    "\n",
    "and so on we want to do this in one pass, where we give the model the correct sentence as the decoder input. It is shifted right, since the model should predict the first token, given the \\<start> token.\n",
    "\n",
    "|                    | Iteration 1                                        |\n",
    "|--------------------|----------------------------------------------------|\n",
    "| **Encoder Input**  | [\"Hello\" ,\"how\", \"are\", \"you\", \"?\"]                |\n",
    "| **Decoder Input**  | [\"\\<start>\", \"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\"] | \n",
    "| **Decoder Output** | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "| **Compare to**     | [\"Hallo\", \"wie\", \"geht's\", \"dir\", \"?\", \"\\<end>\"]   | \n",
    "\n",
    "\n",
    "Problem is, our model could theoretically learn to cheat, by just returning the same sequence it got as an input. In other words, we want to ensure, that when the model is predicting the token [\"geht's\"], it only depends on the previous token [\"\\<start>\", \"Hallo\", \"wie\"]. This can be done with masks!\n",
    "\n",
    "If we look back at our definition of attention we had:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "If we dont want future tokens to have an affect on the current token, we have to ensure, that all scores where $j > i$ are zero! (Normally it would be $\\geq$, but since the decoder input is shifted over by one token - the \\<start> token - its $>$)\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\tilde{x}_0 = s_{00} x_0 + s_{01} x_1 +s_{02} x_2$ \\\n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 +s_{12} x_2$ \\\n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "Now, we want  \n",
    "$\\tilde{x}_0$ to only depend on $x_0$  \n",
    "$\\tilde{x}_1$ to only depend on $x_0$ and $x_1$  \n",
    "$\\tilde{x}_2$ to only depend on $x_0$, $x_1$ and $x_2$  \n",
    "\n",
    "That leads to:  \n",
    "$\\tilde{x}_0 = s_{00} x_0 + 0 \\cdot x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_1 = s_{10} x_0 + s_{11} x_1 + 0 \\cdot x_2$  \n",
    "$\\tilde{x}_2 = s_{20} x_0 + s_{21} x_1 +s_{22} x_2$  \n",
    "\n",
    "The scores form a lower triangle matrix. Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8709b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scores as lower triangular matrix of ones\n",
    "scores = np.tril(np.ones((4, 4)), k=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb4411",
   "metadata": {},
   "source": [
    "Now that we know what we have to do, we have figure when to set these scores to zero! We could try right after we compute the dot products using a mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878371f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mask as lower triangular matrix of ones (same as above)\n",
    "mask = np.tril(np.ones((4, 4)), k=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy embeddings fo this example\n",
    "queries = util.get_dummy_embeddings()\n",
    "keys = util.get_dummy_embeddings()\n",
    "\n",
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680aa112",
   "metadata": {},
   "source": [
    "Looks good! But what happens when we run the softmax over it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa748c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006e35f",
   "metadata": {},
   "source": [
    "Suddenly our weights aren't zero anymore! This shouldn't be that big of a surprise, since $e^0 = 1$.\n",
    "Alright, let's try setting the scores to zero after the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c269f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "scores = queries @ keys.T\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "\n",
    "# Multiply scores with mask to set all scores above the diagonal to zero\n",
    "scores = scores * mask\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab35398",
   "metadata": {},
   "source": [
    "Alright, looks better! Only problem left is that the values don't add up to 1 anymore for every column! We somehow have to change the values before the softmax is applied!\n",
    "That's exactly what infinity mask do! Remember $e^{-inf} = 0$! (Technically it's the limit, but I think you know what we mean!) So these values wouldn't affect the sum, and their value will automatically be zero! So all we have to do is add -inf to the values we want to be zero later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21235cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all zeros with -inf (We want to keep the scores with a one!)\n",
    "inf_mask = np.where(mask, 0, -np.inf)\n",
    "print(inf_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc95f8",
   "metadata": {},
   "source": [
    "Let's add this to our scores before the softmax is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2214367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "scores = (queries @ keys.T) \n",
    "\n",
    "# Add the -inf mask to the scores\n",
    "scores += inf_mask\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = util.softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d55fa6",
   "metadata": {},
   "source": [
    "Perfect! Everything seems to work as planned!\n",
    "\n",
    "Note: This perticular kind of mask is known as a casual mask. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a42e82",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Masked Attention.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Masked Attention.drawio.png\" width=1000>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41b774",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 6: Implement</h3>\n",
    "    <p>Apply masking as explained above in the <code>forward()</code> method of the <code>ScaledDotAttention</code> class in <code>exercise_code/network/attention.py</code> and update the <code>forward()</code> method of the <code>MultiHeadAttention</code> class in <code>exercise_code/network/multi_head_attention.py</code> to pass the mask to the attention heads!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47805b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc01d5e",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "Just like the encoder, the decoder consists of several blocks. \n",
    "\n",
    "Each Block consists of \n",
    "\n",
    "1. **causal multi-head self-attention layer**\n",
    "2. **residual connection**\n",
    "3. **layer normalization**\n",
    "4. **multi-head cross-attention layer**\n",
    "5. **residual connection**\n",
    "6. **layer normalization**\n",
    "7. **feed forward network**\n",
    "8. **residual connection**\n",
    "9. **layer normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603d49d",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-DecoderBlock.drawio.png\" width=\"1500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder Block.drawio.png\" width=1500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386125a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 7: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> method and the <code>forward()</code> method of the <code>DecoderBlock</code> class in <code>exercise_code/network/decoder_block.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4c81",
   "metadata": {},
   "source": [
    "## Decoder Stack\n",
    "\n",
    "Just like in the encoder, the decoder consist of several decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132331",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Decoder.drawio.png\" width=\"1000\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Decoder.drawio.png\" width=1000>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37d1b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 8: Check Code</h3>\n",
    "    <p>Check the <code>__init__()</code> method and the <code>forward()</code> method of the <code>Decoder</code> class in <code>exercise_code/network/decoder.py</code>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9370b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a911f69",
   "metadata": {},
   "source": [
    "## Final Output\n",
    "\n",
    "The last thing we have to do is project the individual embeddings into distributions over our vocabulary. This can be done with a simple linear layer! The outputs at this stage will not be distributions yet, since the values do not add up to one! We accomplish this in the loss layer, using a softmax function!\n",
    "\n",
    "To minimize the weights needed in this network, we will actually use a technique called weight tying! If you remember the Embedding Layer, this was basically a weight matrix with shape (vocab_size, d_model). For our final output layer, we want to project from the embedding space to the vocabulary space, so that gives us (d_model, vocab_size)! The shapes are just transposed! And the approach is to actually reuse these weights from our embeddings and transpose them for our final layer! Here is the corresponding paper: https://arxiv.org/abs/1608.05859v3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd491ec",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/Transformer-Transformer-Full.drawio.png\" width=\"2500\"> -->\n",
    "<img src=\"https://i2dl.vc.in.tum.de/static/images/Transformer-Transformer-Full.drawio.png\" width=2500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5778c96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 9: Implement</h3>\n",
    "    <p>Implement the <code>__init__()</code> and the <code>forward()</code> method of the <code>Transformer</code> class in <code>exercise_code/network/transformer.py</code>. We have already implemented weight tying for you!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569366d6",
   "metadata": {},
   "source": [
    "# Paddings\n",
    "\n",
    "A keen eye might have noticed a problem with our model - in the collate function we added paddings to ensure all sequences have the same length! Our model on the other hand shouldn't change its output just because we are adding \"empty\" tokens at the end of our sequence! The good news is, we have actually already implemented most of what we need to actually enable this! But first, let's have a look at what we are dealing with and load in a batch from our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data import CustomIterableDataset\n",
    "from exercise_code.data import CustomCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path to the dataset\n",
    "file = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "\n",
    "# Define the collator and dataset\n",
    "collator = CustomCollator()\n",
    "dataset = CustomIterableDataset(file)\n",
    "\n",
    "# Define the data loader\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collator)\n",
    "\n",
    "# Create an Embedding layer with 512 dimensions\n",
    "embedding = Embedding(vocab_size=len(collator.tokenizer), d_model=512, max_length=2048)\n",
    "\n",
    "# Get the first batch from the data loader\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c4025",
   "metadata": {},
   "source": [
    "Note: If this didn't work, there is probably a problem in your Dataset! Go Back to Notebook 2 and make sure you pass the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb47e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_masks = batch['encoder_mask']\n",
    "\n",
    "print(padding_masks.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089927b",
   "metadata": {},
   "source": [
    "In this first item of the batch, the sequence has no padding at all - so nothing to do here!\n",
    "For all the others we have come up with something... Let's go back to the formula:\n",
    "\n",
    "$\\tilde{x}_i = \\sum_{j=1}^N s_{ij} x_j$\n",
    "\n",
    "All we really want is that if $x_j$ is a padding token, it doesn't contribute to the updated embedding! In other words - its score has to be zero! We can solve this exactly the same way as we did with the attention masking! Let's do this for a single item, in this case the second item in the batch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7edee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the second item in the batch\n",
    "inputs = embedding(batch['encoder_inputs'][1])\n",
    "\n",
    "# We are using the same embeddings for the queries, keys and values - self-attention!\n",
    "queries = inputs\n",
    "keys = inputs\n",
    "values = inputs\n",
    "\n",
    "print(queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f737bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the padding mask of the second item\n",
    "padding_mask = padding_masks[1].squeeze(0)\n",
    "\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of length 12 by 12 and set the values to 0 where we have to mask\n",
    "mask = torch.ones((len(padding_mask), len(padding_mask)))\n",
    "\n",
    "for i, row in enumerate(mask):\n",
    "    for j, item in enumerate(row):\n",
    "        if not padding_mask[j]:\n",
    "            mask[i, j] = 0\n",
    "    \n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbe5e1",
   "metadata": {},
   "source": [
    "What we did is set the mask to zero, for every column that refers to a padding token! What we get is this matrix, where the left side are ones, and the rest is zero. We can achieve the same result by just copying the vector along the first dimension - duh! (We will let pytorch handle the copying automatically using broadcasting, a keen eye might have noticed that we squeezed the dimension two cells up ;))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d325b6",
   "metadata": {},
   "source": [
    "From here we can treat it the same way we did with our causal attention block, by adding -inf to all values we want to mask out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Compute scores\n",
    "scores = (queries @ keys.T)/np.sqrt(512)\n",
    "\n",
    "# Instead of adding -inf, we set the scores to -inf where the mask is 0 -> Same thing ;)\n",
    "scores.masked_fill_(~mask.bool(), -torch.inf)\n",
    "\n",
    "# Apply softmax to scores\n",
    "scores = softmax(scores, dim=-1)\n",
    "\n",
    "# Print scores\n",
    "scores = scores.detach().numpy()\n",
    "util.plot_attention_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec363f",
   "metadata": {},
   "source": [
    "Perfect, the scores are concentrated to the left side!\n",
    "For the causal mask (the lower triangle mask) we have to combine it with the decoder mask. We can do this by simply multiplying the two together!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75ba46e26ab345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.transformer_util import create_causal_mask\n",
    "\n",
    "# Get the masks from the first batch\n",
    "encoder_mask = batch['encoder_mask']\n",
    "decoder_mask = batch['decoder_mask']\n",
    "\n",
    "# Create the causal mask (lower triangle mask) for the encoder\n",
    "causal_mask = create_causal_mask(decoder_mask.shape[-1])\n",
    "\n",
    "# Combine the decoder mask and the causal mask\n",
    "causal_mask = causal_mask * decoder_mask\n",
    "\n",
    "# Plot all masks\n",
    "util.plot_boolean_masks(causal_mask, encoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664a5816e3591f1",
   "metadata": {},
   "source": [
    "Note: For those who are wondering why the shape dont match up: The decoder mask and encoder mask dont have to be the same length for each sentence ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99655b8d79e74dd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 10: Implement</h3>\n",
    "    <p>Add the padding masks at the appropriate spots in the code! Please go over the <code>forward()</code> passes in the <code>EncoderBlock</code>, <code>DecoderBlock</code> as well as the <code>Transformer</code> class in their respective files!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05c25b7454b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_task_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d54a9d",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    " We employ two types of regularization during training:\n",
    " \n",
    "- Residual Dropout: \n",
    "We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop}$ = 0.1.\n",
    " \n",
    "- Label Smoothing: \n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}$ = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy. This means, that instead of using labels with one-hot encoding:\n",
    "\n",
    "$ y_{1hot} = \\begin{bmatrix}0 & 0 & 0 & \\cdots & 1 & \\cdots & 0 & 0 \\end{bmatrix} $\n",
    "\n",
    "Instead of zeros we use a small value $s = \\frac{\\epsilon_{ls}}{n_{cls} - 1}$, where $n_{cls}$ is the number of classes (=vocab_size). Since this has to be a proper distribution this has to add up 1. This results in a probability of being the correct word $p = 1 - \\epsilon_{ls}$\n",
    "\n",
    "For $n_{cls} = 11$ and $\\epsilon_ls = 0.1$, this would result in:\n",
    "\n",
    "$ y_{smooth} = \\begin{bmatrix}0.01 & 0.01 & 0.01 & 0.01 & 0.9 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f4d18d8abf0ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 11: Implement</h3>\n",
    "    <p>Initialize dropout in the classes <code>Embedding</code>, <code>ScaledDotAttention</code>, <code>MultiHeadAttention</code> and <code>FeedForwardNeuralNetwork</code> in their respective files. Don't forget to add it in the <code>forward()</code> pass! \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5428624bc79f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an Error about SCORE_SAVER - please just restart your kernel!\n",
    "_ = test_task_11()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bf61fd0713dea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 12: Check Code</h3>\n",
    "    <p>Have a look at <code>SmoothCrossEntropy</code> in <code>exercise_code/network/loss.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cdca13956f142",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "The paper used the Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$ and $\\epsilon = 10^{-9}$. </br>\n",
    "They varied the learning rate over the course of training, according to the formula: \n",
    "\n",
    "$lrate = d_{model}^{-0.5} \\cdot min(step\\_num^{−0.5}, step\\_num \\cdot warmup\\_steps^{−1.5})$\n",
    "\n",
    "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae400325ec6e7e1e",
   "metadata": {},
   "source": [
    "The setup could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184f04774015836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exercise_code.data.tokenizer import load_pretrained_fast\n",
    "\n",
    "tokenizer = load_pretrained_fast()\n",
    "model = Transformer(vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "d_model = model.d_model\n",
    "lr_start =d_model**-0.5\n",
    "eps=1e-9\n",
    "betas=(0.9, 0.98)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr_start, eps=eps, betas=betas)\n",
    "\n",
    "warm_up = 4000\n",
    "lr_lambda=lambda step: min((step+1)**-0.5, (step+1)*warm_up**-1.5)\n",
    "scheduler_example = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf3e53da9001a1",
   "metadata": {},
   "source": [
    "Let's have a look at this scheduler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810428a45c4d7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0, 100000)\n",
    "lr = lr_start * np.vectorize(lr_lambda)(steps)\n",
    "\n",
    "plt.plot(steps, lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f53e1d796c23",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Like already mentioned - Training is really where Transformers show there advantages compared to other sequential models! We can use a technique called **Teacher Forcing**, which means we feed the model the correct sentence, and the model only has to predict the next word! This is super easy to do in Transformers due to the causal masks we discussed earlier! In fact, all we have to do is pass the entire correct sentence into the decoder, and let it run exactly one time. The masking will make sure, each predicted word only depends on the previous words - no need to loop!\n",
    "\n",
    "For this exercise, we will not train a large model on a huge dataset, this would simply take too long and also isn't the focus of this exercise! However, you will have to implement the parts of the Trainer class, particular the forward pass! Note that have implemented a couple of extra functionalities, such as training from a checkpoint and a gradient accumulation. Gradient accumulation is used, to decouple the batch size from the optimizing step. Instead of performing an update after every batch, we can update our network after n batches. That way, we can choose a smaller batch size to save RAM, and still have the same effects as we would have with a larger batch! Be aware, that this doesn't work as nicely with batch normalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c47b90e4ad0b77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 13: Implement</h3>\n",
    "    <p>Implement the <code>_forward()</code> method in the Trainer class in <code>exercise_code/trainer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64baecd09e2e9cf",
   "metadata": {},
   "source": [
    "Now we will overfit a small model on a dummy Dataset with small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e404efc259d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.trainer import Trainer\n",
    "from exercise_code.network import SmoothCrossEntropyLoss\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Initialize your tokenizer. You train your own tokenizer and load   #\n",
    "#   load it like we did in the first notebook, or load the pretrained  #\n",
    "#   version!                                                           #\n",
    "#                                                                      #\n",
    "# Hint: Scroll up a couple of cells for the default tokenizer          #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "hparams = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Implement you model here                                           #\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "model = Transformer(vocab_size=len(tokenizer), \n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd36935f75405a",
   "metadata": {},
   "source": [
    "Alright, let's check the model size! For this task, the model should have less than 5 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc89d5eb4b5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_model_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228df541ae40e7a",
   "metadata": {},
   "source": [
    "Alright, now we can define the dataset and the dataloader. The dataset only contains 1000 lines. We will also initialize the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878608289c10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "epochs = None\n",
    "batch_size = None\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "#   Define the optimizer and optionally scheduler - not really needed  #\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n",
    "\n",
    "loss_func = SmoothCrossEntropyLoss(smoothing=0.1)\n",
    "file_path = os.path.join(dataset_path, 'dummyDatasets', 'ds_dummy')\n",
    "collator = CustomCollator(tokenizer=tokenizer)\n",
    "dataset = CustomIterableDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "# For Apple M1/M2/M3 users: Try out the MPS framework, it will significantly speed up your training!\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif MPS_AVAILABLE:\n",
    "    if torch.backends.mps.is_available(): \n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  loss_func=loss_func,\n",
    "                  train_loader=dataloader,\n",
    "                  val_loader=None,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  optimizer_interval=0, # If you want to enable gradient accumulation, you can set this parameter! \n",
    "                  checkpoint_interval=0) # If you want to store your progress. You can resume training using train_from_checkpoint(#folder_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8dd099294456d",
   "metadata": {},
   "source": [
    "Now let's finally train the model! To pass this task, you will need at least 50% accuracy on the dataset! Note: Don't be surprised if this will take a few epochs (>50 probably) and the accuracy will get better very slowly, especially in the beginning! Just let it run, we are overfitting on purpose so it should usually always work at some point!\n",
    "\n",
    "To explain the metrics: The first number is always the loss / accuracy over the current batch and the second number is always computed over the entire epoch.\n",
    "You can also resume training by just executing the cell again. If you reached the end of your epochs and run it again, it will also continue, starting where you left of for as many epochs as you configured. If you stop this cell, your models parameters will not be altered - in other words if you see you have reached the accuracy, just stop the cell ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974182aeee4cda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99929169663da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_and_save_model(trainer, tokenizer, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a96e0dde07757",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The last part to do is test your model in inference! We will feed an input sentence into the algorithm, together with a maximum number of iterations. The sentence is then tokenized, so that we can feed it into the model. The first decoder input will only be the start token, which in our model is the same as the end token! The model will give us an output distribution over all tokens in our vocabulary. We can then either choose the token with the highest probability, or we can sample from this categorical distribution! That way, every answer will be slightly different! The last output is added to the decoder sequence and on it goes! If the model predicts an end token, we stop the algorithm. Otherwise we continue until the maximum number of iterations are reached! Finally, we can decode the output. This will look something like this:\n",
    "\n",
    "\n",
    "Input Sentence: \"Hi how are you\" -> Tokenizer -> Encoder Input: [0, 45, 25, 15, 12, 0]\n",
    "\n",
    "| Iteration  | Encoder Input          | Decoder Input                | Decoder Output                                   |\n",
    "|------------|------------------------|------------------------------|--------------------------------------------------|\n",
    "| 1          | [0, 45, 25, 15, 12, 0] | [0]                          | [445]                                            |\n",
    "| 2          | [0, 45, 25, 15, 12, 0] | [0, 445]                     | [445, 56]                                        |\n",
    "| 3          | [0, 45, 25, 15, 12, 0] | [0, 445, 56]                 | [445, 56, 89]                                    |\n",
    "| 4          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89]             | [445, 56, 89, 76]                                |\n",
    "| 5          | [0, 45, 25, 15, 12, 0] | [0, 445, 56, 89, 76]         | [445, 56, 89, 76, 0]                             | -> 0 Detected!\n",
    "\n",
    "Decoder Output: [445, 56, 89, 76, 0] -> Tokenizer -> Output Sentence: \"Hallo wie geht's dir <[EOS]>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e969f38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 14: Check Code</h3>\n",
    "    <p>Have a look at the <code>predict()</code> method <code>Transformer</code> in <code>exercise_code/network/transformer.py</code>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fcc24c4a3ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, max_iteration_length = 50, probabilistic = False, returns_scores = False):\n",
    "    # Tokenize input sentence\n",
    "    encoder_input = torch.tensor(tokenizer.encode(input_sentence))\n",
    "\n",
    "    # Retrieve output sequence from model\n",
    "    output_sequence, score_records = model.predict(encoder_input, max_iteration_length, probabilistic, returns_scores)\n",
    "\n",
    "    # Decode output sequence\n",
    "    output_sequence = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    \n",
    "    if returns_scores:\n",
    "        return output_sequence, score_records\n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9856c69d65cc8f",
   "metadata": {},
   "source": [
    "Now you can test your model! Feel free to change the variable probabilistic to true! don't be to surprised, if the output is terrible at the moment though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff81d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is on the correct device\n",
    "# Usually the Trainer does this, just in case you stopped training mid epoch!\n",
    "_ = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11848183f60a3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"Hi, how are you today?\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3756c73bdefeaab",
   "metadata": {},
   "source": [
    "Awesome, it works! \n",
    "\n",
    "Well, sort of - This sentece was part of the dataset! Let's try it with a different sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aac04",
   "metadata": {},
   "source": [
    "Well.. ups :D But remember, all we did is overfit to a small dataset with a small model!\n",
    "\n",
    "We have prepared a pretrained model for you! It was trained on a larger dataset and has a lot more parameters! You should be able to load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91548cb2c185c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_pretrained_fast()\n",
    "\n",
    "file_path = os.path.join(pretrained_model_path, 'pretrained_model')\n",
    "load_dict = torch.load(file_path, weights_only=True)\n",
    "model = Transformer(vocab_size=len(tokenizer),\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    hparams=load_dict['hparams'])\n",
    "\n",
    "model.load_state_dict(load_dict['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397a48edee3e416",
   "metadata": {},
   "source": [
    "Now try it again and see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7342c760c205ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = translate(\"This is not part of the dataset!\", probabilistic=False)\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa9e302f00b65c",
   "metadata": {},
   "source": [
    "Alright, seems to work (Apart from a small typo, but we didn't train this model for that long)! Try out probabilistic to see some other results, they might be rubish though...\n",
    "\n",
    "Note: If your model outputs something weird, you probably made some mistake a long the way that we didn't catch! Please go over your Encoder and Decoder Blocks and make sure you did this correctly! Especially look at the residual connections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c4a623d4aa7a8",
   "metadata": {},
   "source": [
    "Congrats! You've now finished your first transformer model! Since this is a totally new exercise, we would really appreciate it if you could give us some [feedback](https://forms.gle/ZUZKcBiSY7bpsQko9)! Like which explanations did you like or not like, what was to hard and maybe what was to easy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dbbe05",
   "metadata": {},
   "source": [
    "To create a zip file with your submission, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff7d9c337e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "path = os.path.join(root_path, 'output', 'exercise12')\n",
    "submit_exercise(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442471dcd3e8742",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.cvg.cit.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.cvg.cit.tum.de/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n",
    "3. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a transformer model!\n",
    "\n",
    "- Points:\n",
    "    - 5 points per Module if shape is correct (FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 5 points per Module if output is correct (FeedForwardNeuralNetwork, EncoderBlock, DecoderBlock, Transformer)\n",
    "    - 30 points if submitted model reaches minimum score\n",
    "    - Total = 4 x 5 + 4 x 5 + 30 = 70\n",
    "\n",
    "- Passing Criteria: Minimum of 65 points!\n",
    "- Feel free to submit an unlimited number of assignments until the end of the semester; however, any submissions made after the deadline will not contribute to your bonus points.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
