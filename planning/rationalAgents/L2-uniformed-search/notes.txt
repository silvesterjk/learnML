

1.1 The design of an agent heavily depends on the type of environment the agent acts upon. We can characterize the types of environments in the following ways:

* In partially observable environments, the agent does not have full information about the state and thus must have an internal estimate of the state of the world. 
* This is in contrast to fully observable environments, where the agent has full information about their state.

* Stochastic environments have uncertainty in the transition model, i.e., taking an action in a specific state may have multiple possible outcomes with different probabilities. 
* This is in contrast to deterministic environments, where taking an action in a state has a single outcome that is guaranteed to happen.

* In multi-agent environments, the agent acts along with other agents. For this reason, the agent might need to randomize its actions to avoid being “predictable” by other agents.
* If the environment does not change as the agent acts on it, it is called static. This is in contrast to dynamic environments that change as the agent interacts with them.

* If an environment has known physics, then the transition model (even if stochastic) is known to the agent, and it can use that when planning a path. 
* If the physics are unknown, the agent will need to take actions deliberately to learn the unknown dynamics.

1.2 State Spaces and Search Problems
https://inst.eecs.berkeley.edu/~cs188/textbook/search/state.html

A search problem consists of the following elements:

1. A state space - The set of all possible states that are possible in your given world
2. A set of actions available in each state
3. A transition model - Outputs the next state when a specific action is taken at current state
4. An action cost - Incurred when moving from one state to another after applying an action
5. A start state - The state in which an agent exists initially
6. A goal test - A function that takes a state as input, and determines whether it is a goal state

1.2.1 State Space Size

Fundamental Counting Principle: 
If there are n variable objects in a given world which can take on  x1, x2, …, xn different values respectively, then the total number of states is  x1, x2, …, xn

Image: https://inst.eecs.berkeley.edu/~cs188/textbook/assets/images/state_space_size.png

If the variable objects and their corresponding number of possibilities are as follows:

Pacman positions - Pacman can be in 120 distinct (x,y) positions, and there is only one Pacman
Pacman Direction - this can be North, South, East, or West, for a total of 4 possibilities
Ghost positions - There are two ghosts, each of which can be in 12 distinct (x,y) positions
Food pellet configurations - There are 30 food pellets, each of which can be eaten or not eaten

Using the fundamental counting principle:
- We have 120 positions for Pacman
- 4 directions Pacman can be facing
- 12 · 12 ghost configurations (12 for each ghost)
- 2 · 2 · . . . · 2 = 230 food pellet configurations (each of 30 food pellets has two possible values - eaten or not eaten). 

This gives us a total state space size of 120· 4 · 12^2 · 2^30

1.2.2 State Space Graphs and Search Trees

A state space graph is constructed with states representing nodes, with directed edges existing from a state to its children. 
These edges represent actions, and any associated weights represent the cost of performing the corresponding action. 

https://inst.eecs.berkeley.edu/~cs188/textbook/assets/images/graph_and_tree.png

The highlighted path (S → d → e → r → f → G) in the given state space graph is represented in the corresponding search tree by following the path in the tree from the start state S to the highlighted goal state G.

1.3 Uninformed Search
https://inst.eecs.berkeley.edu/~cs188/textbook/search/uninformed.html

When we have no knowledge of the location of goal states in our search tree, we are forced to select our strategy for tree search from one of the techniques that falls under the umbrella of uninformed search. We’ll now cover three such strategies in succession: depth-first search, breadth-first search, and uniform cost search. 

Along with each strategy, some rudimentary properties of the strategy are presented as well, in terms of the following:
- The completeness of each search strategy - if there exists a solution to the search problem, is the strategy guaranteed to find it given infinite computational resources?
- The optimality of each search strategy - is the strategy guaranteed to find the lowest cost path to a goal state?
- The branching factor b - The increase in the number of nodes on the frontier each time a frontier node is dequeued and replaced with its children is O(b). 
- At depth k in the search tree, there exists  O(b^k) nodes.
- The maximum depth m.
- The depth of the shallowest solution s.

1.3.1 Depth-First Search
Depth-first search (DFS) is a strategy for exploration that always selects the deepest frontier node from the start node for expansion.

https://inst.eecs.berkeley.edu/~cs188/textbook/assets/images/dfs.png

1.3.2 Breadth-First Search
Breadth-first search is a strategy for exploration that always selects the shallowest frontier node from the start node for expansion.

https://inst.eecs.berkeley.edu/~cs188/textbook/assets/images/bfs.png


1.3.3 Uniform Cost Search
Uniform cost search (UCS) is a strategy for exploration that always selects the lowest cost frontier node from the start node for expansion.

- The choice is usually a heap-based priority queue, where the priority for a given enqueued node v is the path cost from the start node to v or the backward cost of v. 
- A priority queue constructed in this manner simply reshuffles itself to maintain the desired ordering by path cost as we remove the current minimum cost path and replace it with its children.
