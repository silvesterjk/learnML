{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion vs Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.3.21\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion models has been deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(input=\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Processing - Make multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nWhy did the cow go to outer space? To find the MOO-n!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the parrot wear a raincoat?\\n\\nBecause he wanted to be polly-protected!', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 38, 'total_tokens': 51, 'prompt_tokens': 13}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('022c2b17-73a1-4b12-b93a-562fe59f0f97')), RunInfo(run_id=UUID('5b692e44-0351-4499-ae93-5f694b0da25a'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "result = llm.generate([\"Tell me a joke about cows\", \"Tell me a joke about parrots\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n  \"generations\": [\\n    {\\n      \"text\": \"\\n\\nWhy did the cow cross the road?\\n\\nTo get to the other side!\",\\n      \"generation_info\": {\\n        \"finish_reason\": \"stop\",\\n        \"logprobs\": null\\n      }\\n    },\\n    {\\n      \"text\": \"\\n\\nWhy did the parrot wear a raincoat?\\n\\nBecause he wanted to be polly-proof!\",\\n      \"generation_info\": {\\n        \"finish_reason\": \"stop\",\\n        \"logprobs\": null\\n      }\\n    }\\n  ],\\n  \"llm_output\": {\\n    \"token_usage\": {\\n      \"completion_tokens\": 38,\\n      \"prompt_tokens\": 13,\\n      \"total_tokens\": 51\\n    },\\n    \"model_name\": \"gpt-3.5-turbo-instruct\"\\n  },\\n  \"run\": [\\n    {\\n      \"run_id\": \"9c1c3f25-4b16-4231-9ad7-491b7992a13f\"\\n    },\\n    {\\n      \"run_id\": \"396c42c9-ec90-48c8-aa28-2fe2fade98f3\"\\n    }\\n  ],\\n  \"type\": \"LLMResult\"\\n}\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{\n",
    "  \"generations\": [\n",
    "    {\n",
    "      \"text\": \"\\n\\nWhy did the cow cross the road?\\n\\nTo get to the other side!\",\n",
    "      \"generation_info\": {\n",
    "        \"finish_reason\": \"stop\",\n",
    "        \"logprobs\": null\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"\\n\\nWhy did the parrot wear a raincoat?\\n\\nBecause he wanted to be polly-proof!\",\n",
    "      \"generation_info\": {\n",
    "        \"finish_reason\": \"stop\",\n",
    "        \"logprobs\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"llm_output\": {\n",
    "    \"token_usage\": {\n",
    "      \"completion_tokens\": 38,\n",
    "      \"prompt_tokens\": 13,\n",
    "      \"total_tokens\": 51\n",
    "    },\n",
    "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
    "  },\n",
    "  \"run\": [\n",
    "    {\n",
    "      \"run_id\": \"9c1c3f25-4b16-4231-9ad7-491b7992a13f\"\n",
    "    },\n",
    "    {\n",
    "      \"run_id\": \"396c42c9-ec90-48c8-aa28-2fe2fade98f3\"\n",
    "    }\n",
    "  ],\n",
    "  \"type\": \"LLMResult\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 38,\n",
       "  'total_tokens': 51,\n",
       "  'prompt_tokens': 13},\n",
       " 'model_name': 'gpt-3.5-turbo-instruct'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a conversation with SystemMessage, AIMessage and HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What do you call a cow with no legs? \\n\\nGround beef! 🐄😄' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 13, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-Bfh739TmBbNRlk14eWusQzjulnT4O', 'finish_reason': 'stop', 'logprobs': None} id='run-e62ff689-10a8-4efd-84bc-72d7196e2335-0' usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# predict to invoke --> Based on the changes introduced in langchain 0.2.0\n",
    "result = llm.invoke(\"Tell me a joke about cows\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant specialized in providing information about BellaVista Italian Restaurant.\"),\n",
    "    HumanMessage(content=\"What's on the menu?\"),\n",
    "    AIMessage(content=\"BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood.\"),\n",
    "    HumanMessage(content=\"Do you have vegan options?\"),\n",
    "    AIMessage(content=\"BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Yes, BellaVista typically has vegan options on their menu. These may include vegan pasta dishes, salads, and pizzas that can be customized without cheese. However, the specific offerings can vary by location, so it's always a good idea to check with the restaurant directly or look at their menu online for the most current options.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 81, 'total_tokens': 145, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bfh7464Ybd7tHEeml477jbckLVA1p', 'finish_reason': 'stop', 'logprobs': None}, id='run-58e4660f-1fae-45dc-83d1-a2c1b2a44bb9-0', usage_metadata={'input_tokens': 81, 'output_tokens': 64, 'total_tokens': 145, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.invoke(input=messages)\n",
    "llm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Processing for Chat Models - Make multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to Kannada\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates the English to Hindi.\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "]\n",
    "batch_result = llm.generate(batch_messages)\n",
    "batch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LLMResult is the top-level object that contains a list of generations.\n",
    "2. Each of those elements is itself a list of Generation (or ChatGeneration) objects.\n",
    "3. The code expects batch_result to be an LLMResult object that has a .generations attribute (which is a list of lists of Generation objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.outputs.llm_result.LLMResult'>\n"
     ]
    }
   ],
   "source": [
    "print(type(batch_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbatch_result = LLMResult(\\n    generations=[\\n        # First input's generations\\n        [ChatGeneration(text='ನಿಮ್ಮ ಬಳಿ ಶಾಕಾಹಾರಿ ಆಯ್ಕೆಗಳು ಇದಿವೆಯಾ?', ...)], # This is generation[0] for the first batch item\\n        # Second input's generations\\n        [ChatGeneration(text='क्या आपके पास शाकाहारी विकल्प हैं?', ...)]  # This is generation[0] for the second batch item\\n    ],\\n    # ... other LLMResult attributes\\n)\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch_result = LLMResult(\n",
    "    generations=[\n",
    "        # First input's generations\n",
    "        [ChatGeneration(text='ನಿಮ್ಮ ಬಳಿ ಶಾಕಾಹಾರಿ ಆಯ್ಕೆಗಳು ಇದಿವೆಯಾ?', ...)], # This is generation[0] for the first batch item\n",
    "        # Second input's generations\n",
    "        [ChatGeneration(text='क्या आपके पास शाकाहारी विकल्प हैं?', ...)]  # This is generation[0] for the second batch item\n",
    "    ],\n",
    "    # ... other LLMResult attributes\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ನಿಮ್ಮ ಬಳಿ ಶ್ರೇಷ್ಟವಾದ ಆಯ್ಕೆಗಳು ಇವೆಯೆ?', 'क्या आपके पास शाकाहारी विकल्प हैं?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = [gen_text[0].text for gen_text in batch_result.generations]\n",
    "translations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
